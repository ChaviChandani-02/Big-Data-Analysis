# Big-Data-Analysis

COMPANY- CODTECH IT SOLUTIONS

NAME- CHAVI CHANDANI

INTERN ID- CT04DA482

DOMAIN- DATA ANALYTICS

DURATION- 4 WEEKS

MENTOR- NEELA SANTOSH


# NETFLIX VIEWERSHIP ANALYSIS USING PYSPARK
# OVERVIEW

This project focuses on analyzing a large- scale Netflix dataset using PySpark, a distributed data processing framework.
The goal was to derive meaningful insights from the data, including trends in content popularity, regional viewership, and category- based preferences.
Using scalable big data tools, the analysis demonstrates how massive datasets can be processed to uncover patterns and support strategic decision- making

# Tools and Technologies Used:
1. Apache Spark (PySpark):
PySpark is the Python API for Apache Spark, used for fast and distributed processing of large datasets.
It enables parallel computation, making it ideal for big data tasks across multiple machines.
Used handling large datasets with distributed processing and DataFrame transformations.
   

2. Google Colab:
Google Colab is a free, cloud-based platform to write and run Python code in notebooks.
It provides access to powerful GPUs and TPUs, which help process large datasets efficiently.


3. CSV (Comma-Separated Values):
CSV is a common format used to store and share tabular data in plain text.
The dataset format used, loaded using spark.read.csv() for structured processing.
 
4. Matplotlib & Seaborn:
Used for data visualization—creating bar charts, line plots, and trend graphs for better interpretability.


  
5. Python:
Python is a widely used programming language in data science and machine learning.
It was used in this task for data handling, visualization, and integration with PySpark.
Python’s simplicity and vast library support made it perfect for building the analysis workflow.


   
# Real-World Applications of This Task:
1. Netflix Viewership Analysis:
This type of analysis is relevant for businesses like Netflix to understand viewership trends. It helps determine which shows are most popular, how 
different categories perform, and which countries have the highest number of viewers.

2. Content Strategy:
Helps streaming platforms identify what types of content are performing well across different countries.

3. Marketing and Personalization:
Supports targeted marketing and regional recommendations based on user trends and preferences.

4. Business Insights:
Assists Netflix and similar platforms in making data-driven decisions about new releases and global expansions.

   


6. Scalable analytics:
Showcases how companies can process vast amounts of data without lag using scalable tools like PySpark.



# Key Insights to Derive:

1. Identified the Top 10 most frequent shows that consistently appeared in the Top 10 rankings globally
2. Discovered the most popular categories, showing viewer preferences in terms of genres or types of content
3. Highlighted top- performing countries by frequency of content appearance, giving insight into regional engagement levels
4. Extracted countries with the highest number of unique titles, showing diversity in content consumption across nations
5. Displayed a line plot of the most viewed shows, highlighting long-term audience interest and show stickiness.


# Learning Outcomes:
1. Demonstrated efficient analysis of a large dataset using PySpark and cloud tools without memory issues

2. Successfully created visualizations that reveal trends in content popularity, viewer geography, and category engagement

3. Gained hands-on experience with Big Data tools and workflows, aligning with real-world data analytics practices

4. Delivered a clean, insightful, and scalable notebook/script that highlights the power of distributed computing for media analytics



# What i learned
1. How to use PySpark DataFrames and SQL functions to process and transform large datasets.

2. The importance of data cleaning and grouping to uncover actionable trends.

3. How data visualization enhances storytelling and makes insights more accessible.

4. Realized the value of scalable platforms like Spark in industries that work with high-volume data daily.



